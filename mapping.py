import unicodedata
import re
import random
from itertools import product

LEET_MAP = {
    "0": "o", "1": "i", "3": "e", "4": "a", "5": "s", "6": "g", "7": "t", "8": "b", "9": "g",
    "$": "s", "@": "a", "!": "i", "â‚¬": "e", "Â£": "l", "Â¥": "y", "Â§": "s"
}

HOMOGLYPHS = {
    "a": ["Ð°", "ðš", "ð–†", "áµƒ", "â“"], "b": ["ð›", "ð–‡", "áµ‡"], "c": ["Ñ", "ðœ", "ð–ˆ", "á¶œ"],
    "d": ["ð", "ð–‰", "áµˆ"], "e": ["Ðµ", "ðž", "ð–Š", "áµ‰", "â“”"], "f": ["ðŸ", "ð–‹", "â“•"],
    "g": ["ð ", "ð–Œ"], "h": ["ð¡", "ð–"], "i": ["Ñ–", "ð¢", "ð–Ž", "á¶¦", "â“˜"], "j": ["ð£", "ð–"],
    "k": ["ð¤", "ð–"], "l": ["ð¥", "ð–‘", "â“›"], "m": ["ð¦", "ð–’"], "n": ["ð§", "ð–“", "â“"],
    "o": ["Ð¾", "ð¨", "ð–”", "áµ’", "â“ž"], "p": ["Ñ€", "ð©", "ð–•"], "q": ["ðª", "ð––"],
    "r": ["ð«", "ð–—", "â“¡"], "s": ["Ñ•", "ð¬", "ð–˜", "áµ—Ë¢", "â“¢"], "t": ["ð­", "ð–™", "â“£"],
    "u": ["Ï…", "ð®", "ð–š", "áµ˜", "â“¤"], "v": ["ð¯", "ð–›", "â“¥"], "w": ["ð°", "ð–œ", "â“¦"],
    "x": ["Ñ…", "ð±", "ð–", "â“§"], "y": ["Ñƒ", "ð²", "ð–ž", "â“¨"], "z": ["ð³", "ð–Ÿ", "â“©"]
}

ZERO_WIDTH_CHARS = ["\u200b", "\u200c", "\u200d", "\ufeff"]
SPLITTER_CHARS = [".", "-", "_", "*", "/", "\\"]

def normalize_word(w: str) -> str:
    w = unicodedata.normalize("NFKD", w)
    w = "".join(LEET_MAP.get(c, c.lower()) for c in w)
    w = re.sub(r"[^\w]", "", w)
    w = re.sub(r"(.)\1{2,}", r"\1", w)
    return w

def apply_homoglyphs(w: str) -> str:
    return "".join(random.choice(HOMOGLYPHS.get(c, [c])) if random.random() < 0.5 else c for c in w)

def apply_zero_width(w: str) -> str:
    out = []
    for c in w:
        out.append(c)
        if random.random() < 0.4:
            out.append(random.choice(ZERO_WIDTH_CHARS))
    return "".join(out)

def apply_splitters(w: str) -> str:
    return random.choice(SPLITTER_CHARS).join(list(w))

def apply_fullwidth(w: str) -> str:
    return "".join(chr(ord(c)+0xFEE0) if '!' <= c <= '~' else c for c in w)

def generate_variants(word: str, max_variants: int = 10):
    word = normalize_word(word)
    variants = set()
    variants.add(word)
    transformations = [
        apply_homoglyphs,
        apply_zero_width,
        apply_splitters,
        apply_fullwidth,
        lambda x: x.translate(str.maketrans("aegiost", "4361057"))
    ]
    for t in transformations:
        v = t(word)
        variants.add(normalize_word(v))
        if len(variants) >= max_variants:
            break
    return variants

def split_text(text: str):
    raw_tokens = re.findall(r"\b[\w\.\!\$\@\#\%\&\*\-]+\b", text)
    normalized = set()
    for t in raw_tokens:
        n = normalize_word(t)
        if n:
            normalized.add(n)
    return list(normalized)

def expand_tokens(tokens: list):
    all_variants = set()
    for tok in tokens:
        all_variants.update(generate_variants(tok))
    return list(all_variants)
